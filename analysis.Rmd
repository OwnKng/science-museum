# Using Text Analysis to Classify Exhibits in the Science Museum 

The Science Museum holds more than 325,000 objects in its exhibition halls and archives, including the world's oldest surviving locomotive, the first jet engine and the Apollo 10 command module. The Science Museum have made available information about each object in their collection via an [api](https://www.sciencemuseumgroup.org.uk/about-us/collection/using-our-collection-api/), which provides details of the object's age, country of origin, materials and a text description. 

In this article, we're going to explore these text descriptions using the `tidytext()` package, and then build a machine learning model using using the `tidymodels()`. This machine learning model will predict use the description of the object to identify whether it is an item from the the computing and data collection, or the space collection.     

We'll start by loading the `tidyverse()` packages, which we'll be using throughout our analysis, and reading in the data. I've written a separate script to query the api and export the descriptions as a csv, which is available on my github. 

```{r, include=FALSE}
options(scipen = 9999)
```

```{r}
library(tidyverse)

computing <- read_csv("data/computing-data-processing/objects.csv") 
space <- read_csv("data/space/objects.csv")

objects <- bind_rows(list('computing' = computing, 'space' = space), .id = 'category')

objects

```
```{r}
library(tidytext)

object_descriptions <- objects %>% 
  unnest_tokens('word', description) %>% 
  anti_join(stop_words, by = 'word') %>% 
  filter(!str_detect(word, "[0-9]"))

object_descriptions
```

```{r}
object_descriptions %>% 
  add_count(id) %>% 
  distinct(id, n) %>% 
  ggplot(aes(n)) +
  geom_histogram(binwidth = 5, boundary = 0, color = 'white') + 
  geom_rug()

```

Which words are most common? 
```{r}
object_descriptions %>% 
  count(category, word) %>% 
  group_by(category) %>% 
  slice_max(n = 20, order_by = n) %>% 
  mutate(word = reorder_within(word, n, category)) %>% 
  ggplot(aes(n, word)) + 
  geom_col() + 
  facet_wrap(~category, scales = 'free') + 
  scale_y_reordered()

```

```{r}
library(tidylo)

objects_log_odds <- object_descriptions %>% 
  count(category, word) %>% 
  bind_log_odds(category, word, n) %>% 
  arrange(desc(log_odds_weighted)) 

objects_log_odds %>% 
  group_by(category) %>% 
  slice_max(log_odds_weighted, n = 20)  %>% 
  mutate(word = fct_reorder(word, log_odds_weighted)) %>% 
  ggplot(aes(log_odds_weighted, word, fill = category)) + 
  geom_col() + 
  facet_wrap(~category, scales = "free")

```

How are the words connected? 

```{r}
library(widyr)

word_pairs <- object_descriptions %>% 
  group_by(category) %>% 
  nest() %>% 
  mutate(pairs = map(data, pairwise_count, word, id, sort = TRUE, upper = FALSE)) %>% 
  select(-data) %>% 
  unnest(pairs) %>% 
  ungroup()

```

```{r}
library(ggraph)
library(igraph)

set.seed(42)

word_pairs %>% 
  filter(category == 'computing', n > 30) %>%
  select(-category) %>% 
  graph_from_data_frame() %>% 
  ggraph(layout = 'fr') +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) + 
  theme_void()

word_pairs %>% 
  filter(category == 'space', n > 10) %>%
  select(-category) %>% 
  graph_from_data_frame() %>% 
  ggraph(layout = 'fr') +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "hotpink") +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) + 
  theme_void()

```

```{r}
keywords <- object_descriptions %>% 
  group_by(category) %>% 
  add_count(word) %>% 
  ungroup() %>% 
  filter(category == 'computing' & n > 20 | category == 'space' & n > 10) 

word_cors <- keywords %>% 
  group_by(category) %>% 
  nest() %>% 
  mutate(cor = map(data, pairwise_cor, word, id, sort = TRUE, upper = FALSE)) %>% 
  select(-data) %>% 
  unnest(cor) %>% 
  ungroup()

word_cors %>% 
  filter(category == 'computing', correlation > 0.5) %>%
  select(-category) %>% 
  graph_from_data_frame()  %>% 
  ggraph(layout = 'fr') +
  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = "cyan4") +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) + 
  theme_void()

word_cors %>% 
  filter(category == 'space', correlation > 0.3) %>%
  select(-category) %>% 
  graph_from_data_frame()  %>% 
  ggraph(layout = 'fr') +
  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = "hotpink") +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) + 
  theme_void()

```

Building a model

Separate our data into testing and training data.  
```{r}
library(tidymodels)

set.seed(42)

objects_split <- initial_split(objects, strata = category)
objects_training <- training(objects_split)
objects_testing <- testing(objects_split)

```

Create a set of cross-validation folds, which will divide our training data into 10 datasets, each containing an analysis and assessment set.  

```{r}
set.seed(42)
objects_folds <- vfold_cv(objects_training, strata = category)
objects_folds

```

Create our model recipe
```{r}
library(textrecipes)
library(themis)

additional_stop_words <- objects %>%
  unnest_tokens('word', description) %>% 
  filter(str_detect(word, "[0-9]")) %>% 
  pull(word)

model_recipe <- recipe(category ~ description, data = objects_training) %>% 
  step_tokenize(description) %>% 
  step_stopwords(description, custom_stopword_source = additional_stop_words) %>% 
  step_stopwords(description) %>% 
  step_stem(description) %>% 
  step_tokenfilter(description, max_tokens = 500) %>% 
  step_tfidf(description) %>% 
  step_downsample(category) 

```

```{r}
prep(model_recipe) %>% 
  bake(new_data = NULL) 

```


Create a workflow - workflows allow us to construct a model from the recipe and the model spec. 
```{r}
object_workflow <- workflow() %>% 
  add_recipe(model_recipe)

```

collect_metrics() returns the summarised metrics from our folds - in our case the mean. 

```{r}
null_classification <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")

null_rs <- workflow() %>%
  add_recipe(model_recipe) %>%
  add_model(null_classification) %>%
  fit_resamples(objects_folds)

null_rs %>% 
  collect_metrics()
```


```{r}
library(discrim)

nb_spec <- naive_Bayes() %>% 
  set_mode('classification') %>% 
  set_engine('naivebayes')

object_workflow <- object_workflow %>% 
  add_model(nb_spec)

```

```{r}

nb_model <- fit_resamples(
  object_workflow,
  objects_folds,
  metrics = metric_set(accuracy, sensitivity, specificity)
)

collect_metrics(nb_model)

```

Random Forest Model

```{r}
rf_spec <- rand_forest(trees = 1000) %>%
  set_engine("randomForest") %>%
  set_mode("classification")

object_workflow <- object_workflow %>% 
  update_recipe(model_recipe) %>% 
  update_model(rf_spec) 

```

```{r}
set.seed(2021)

rf_model <- fit_resamples(
  object_workflow,
  objects_folds,
  metrics = metric_set(accuracy, sensitivity, specificity)
)

rf_model %>% 
  collect_metrics()

```

Creating a support-vector machine model 
```{r}
svm_spec <- svm_rbf() %>%
  set_mode("classification") %>%
  set_engine("liquidSVM")

svm_spec
```

```{r}
object_workflow <- object_workflow %>%
  update_model(svm_spec)

object_workflow
```

```{r}
set.seed(2021)

svm_rs <- fit_resamples(
  object_workflow,
  objects_folds,
  metrics = metric_set(accuracy, sensitivity, specificity),
)

```

```{r}
svm_rs_metrics <- collect_metrics(svm_rs)
```

```{r}
svm_rs_metrics
```

Tuning our SVM model. 

```{r}
svm_spec_tuned <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>% 
  set_mode('classification') %>% 
  set_engine('liquidSVM')

```

```{r}
param_grid <- grid_regular(cost(), rbf_sigma(), levels = 5)

```

```{r}
object_workflow <- object_workflow %>% 
  update_model(svm_spec_tuned)

```


```{r}
set.seed(2021)

tune_rs <- tune_grid(
  object_workflow,
  objects_folds,
  grid = param_grid,
  metrics = metric_set(accuracy, sensitivity, specificity),
  control = control_resamples(save_pred = TRUE)
)

```

```{r}
collect_metrics(tune_rs) %>% 
  filter(.metric == 'accuracy') %>% 
  arrange(desc(.metric))

show_best(tune_rs, metric = "accuracy")

```

```{r}
best_accuracy <- select_best(tune_rs, 'accuracy')

object_workflow_final <- finalize_workflow(object_workflow, best_accuracy)

object_workflow_final

```

```{r}
final_res <- object_workflow_final %>% 
  last_fit(objects_split, metrics = metric_set(accuracy, sensitivity, specificity))
```

```{r}
final_res_metrics <- collect_metrics(final_res)
final_res_predictions <- collect_predictions(final_res)
```

```{r}
final_res_metrics
```































